In order to train the models apart from their building blocks, it is necessary to define the process that the network is going to learn throughout the training. However, because of the large number of parameters, choosing the right weights for the model and in a relatively quick time can be a quite challenging task for the builder of the network. \\~\\
For this reason, the use of an optimization algorithm (optimizer) was decided, so that it modifies the characteristics of the neural network, such as the weights and the learning rate, leading to an immediate reduction of total loss and improving accuracy. The optimizer used in this case is Adam with one relatively low learning rate, set as 0.001.